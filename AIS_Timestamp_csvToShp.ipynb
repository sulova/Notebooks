{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pathlib \n",
    "from pathlib import Path\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import fiona\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "from datetime import date\n",
    "from dateutil.rrule import rrule, DAILY\n",
    "\n",
    "import shutil \n",
    "from urllib.request import URLopener\n",
    "import urllib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert input parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = date(2020, 11, 10)\n",
    "b = date(2020, 11, 20)\n",
    "\n",
    "ship_name = \"Rosalie\"\n",
    "\n",
    "path = '/ncr2420/ANSU/6_Tasks/2109_ShipDetection/4_LotteRosalie_14_20_November_2020/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dowloading the AIS data from the Danish Marine Authority**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7aa9e277de74ae880911bf8debc0846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Number files to read', max=11, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file exists: 20201110.\n",
      "The file exists: 20201111.\n",
      "The file exists: 20201112.\n",
      "The file exists: 20201113.\n",
      "The file exists: 20201114.\n",
      "The file exists: 20201115.\n",
      "The file exists: 20201116.\n",
      "The file exists: 20201117.\n",
      "The file exists: 20201118.\n",
      "The file exists: 20201119.\n",
      "The file exists: 20201120.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mainlist = []\n",
    "\n",
    "for dt in rrule(DAILY, dtstart=a, until=b):\n",
    "    mainlist.append(dt.strftime(\"%Y%m%d\"))\n",
    "\n",
    "for i in tqdm(mainlist, desc='Number files to read'):\n",
    "    path_full = path + i +'.csv'\n",
    "    if Path(path_full).is_file():\n",
    "        print(f'The file exists: {i}.')\n",
    "    else:    \n",
    "        url = 'ftp://ftp.ais.dk/ais_data/aisdk_'+ i +'.csv'\n",
    "        urllib.request.urlretrieve(url, path_full)\n",
    "        urllib.request.urlcleanup()\n",
    "        print('The file was downloaded:',path_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter based on a ship name and save it as a csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69cc6acbcb743aa8d6f93257dc2e3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Number files to read', max=22, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected file exit for:20201110\n",
      "Selected file exit for:20201111\n",
      "Selected file exit for:20201112\n",
      "Selected file exit for:20201113\n",
      "Selected file exit for:20201114\n",
      "Selected file exit for:20201115\n",
      "Selected file exit for:20201116\n",
      "Selected file exit for:20201117\n",
      "Selected file exit for:20201118\n",
      "Selected file exit for:20201119\n",
      "Selected file exit for:20201120\n"
     ]
    }
   ],
   "source": [
    "paths = pathlib.Path(path).rglob('*.csv')\n",
    "paths_list = list(paths) \n",
    "\n",
    "file_names = []\n",
    "file_names_selected = []\n",
    "\n",
    "for f in tqdm(paths_list,desc='Number files to read'):\n",
    "    if \"selected\" in f.name:\n",
    "        file_names_selected.append(f.name[:-4])\n",
    "    elif not \"selected\" in f.name:\n",
    "        file_names.append(f.name[:-4])\n",
    "        \n",
    "for file in file_names:\n",
    "    if not file + \"_selected\" in file_names_selected:\n",
    "        print(f'Selected file does not exit for:{file}')\n",
    "        df = pd.read_csv(f'{path}{file}.csv')\n",
    "        df_selected = df[df['Name'].str.contains(ship_name,na=False)]\n",
    "        df_selected.to_csv(f'{path+file}_selected.csv')\n",
    "    else:\n",
    "       print(f'Selected file exit for:{file}') \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unique ship name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all unique vessels names\n",
    "df = pd.read_csv(f'{path}{file}.csv')\n",
    "unique_names = df['Name'].unique()\n",
    "\n",
    "# for i in unique_names:print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print Timestamp from selected files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20201110_selected.csv\n",
      "NaT   -   NaT\n",
      "20201111_selected.csv\n",
      "NaT   -   NaT\n",
      "20201112_selected.csv\n",
      "NaT   -   NaT\n",
      "20201113_selected.csv\n",
      "NaT   -   NaT\n",
      "20201114_selected.csv\n",
      "NaT   -   NaT\n",
      "20201115_selected.csv\n",
      "NaT   -   NaT\n",
      "20201116_selected.csv\n",
      "NaT   -   NaT\n",
      "20201117_selected.csv\n",
      "NaT   -   NaT\n",
      "20201118_selected.csv\n",
      "NaT   -   NaT\n",
      "20201119_selected.csv\n",
      "NaT   -   NaT\n",
      "20201120_selected.csv\n",
      "NaT   -   NaT\n"
     ]
    }
   ],
   "source": [
    "# Search for files named with a suffix of selected\n",
    "paths_selected = pathlib.Path(path).rglob('*_selected.csv')\n",
    "paths_list_selected = list(paths_selected)  \n",
    "\n",
    "for f in paths_list_selected:\n",
    "    print(f.name)                                                                       \n",
    "    df = pd.read_csv(f'{f}')\n",
    "    df['dt'] = pd.to_datetime(df['# Timestamp'])\n",
    "    df_Timestamp_max  = df['dt'].max()\n",
    "    df_Timestamp_min  = df['dt'].min()\n",
    "    print(df_Timestamp_max, '  -  ' ,df_Timestamp_min)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create shapfiles from csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing your .csv files\n",
    "\n",
    "paths_selected = pathlib.Path(path).rglob('*_selected.csv')\n",
    "paths_list_selected = list(paths_selected)  \n",
    "\n",
    "# Read files sequentially\n",
    "for file in paths_list_selected:\n",
    "    df = pd.read_csv(file)          #Reading your csv file with pandas\n",
    "        \n",
    "    # Create tuples of geometry by zipping Longitude and latitude columns in your csv file\n",
    "    geometry = [Point(xy) for xy in zip(df.Longitude, df.Latitude)] \n",
    "        \n",
    "    # Define coordinate reference system on which to project your resulting shapefile\n",
    "    crs = {'init': 'epsg:4326'}\n",
    "        \n",
    "    # Convert pandas object (containing your csv) to geodataframe object using geopandas\n",
    "    gdf = GeoDataFrame(df, crs = crs, geometry=geometry)\n",
    "        \n",
    "    # Save file to local destination\n",
    "    gdf.to_file(filename=str(file)[:-4]+'.shp', driver='ESRI Shapefile')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the timestamp period:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['dt'] = pd.to_datetime(df['# Timestamp'])\n",
    "# df_selected = df[(df['dt'] > '2021-01-09 05:44:59') & (df['dt'] < '2021-01-09 05:55:01')]\n",
    "# df_selected = df[(df['Name'] = 'Jolissa') \n",
    "# df[df['ids'].str.contains(\"ball\")]\n",
    "#df_selected.to_csv('/ncr2420/ANSU/6_Tasks/2109_ShipDetection/aisdk_20200605_selected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (DHI GRAS)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
